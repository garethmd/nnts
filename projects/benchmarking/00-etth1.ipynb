{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import nnts\n",
    "import nnts.data\n",
    "from nnts import utils\n",
    "import nnts.torch.models\n",
    "import nnts.torch.trainers\n",
    "import nnts.metrics\n",
    "import nnts.torch.datasets\n",
    "import nnts.torch.utils\n",
    "import nnts.loggers\n",
    "from nnts import datasets\n",
    "torch.set_printoptions(precision=8, sci_mode=False)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "model_name = \"\"\n",
    "dataset_name = \"ETTh1\"\n",
    "results_path = \"nb-results\"\n",
    "metadata_path = \"informer.json\"\n",
    "\n",
    "metadata = datasets.load_metadata(dataset_name, path=metadata_path)\n",
    "datafile_path = os.path.join(data_path, metadata.filename)\n",
    "PATH = os.path.join(results_path, model_name, metadata.dataset)\n",
    "df = pd.read_csv(datafile_path)\n",
    "utils.makedirs_if_not_exists(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = utils.Hyperparams(\n",
    "#     optimizer=torch.optim.Adam,\n",
    "#     loss_fn=torch.nn.L1Loss(),\n",
    "#     batch_size=32,\n",
    "#     batches_per_epoch=50,\n",
    "#     training_method=utils.TrainingMethod.DMS,\n",
    "#     model_file_path=\"logs\",\n",
    "#     epochs=100,\n",
    "#     scheduler=utils.Scheduler.REDUCE_LR_ON_PLATEAU,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename({\"date\": \"ds\", \"OT\": \"y\"}, axis=\"columns\")\n",
    "df['unique_id'] = 'T1'\n",
    "#df = df[['unique_id', 'ds', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY RUN FOR MULTIVARIATE\n",
    "#df = pd.melt(df, id_vars=[\"date\"], value_vars=[\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"], var_name=\"unique_id\", value_name=\"y\")\n",
    "#df = df.rename({\"date\": \"ds\"}, axis=\"columns\")\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split lengths as per informer\n",
    "# trn_length = int(24 * 365.25)\n",
    "# val_test_length = int(24 * 365.25 * (4 / 12))\n",
    "nnts.torch.utils.seed_everything(2021)\n",
    "trn_length = 8640\n",
    "val_test_length = 2880\n",
    "split_data = datasets.split_test_val_train(\n",
    "    df, trn_length, val_test_length, val_test_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nnts.torch.datasets\n",
    "import nnts.torch.models.dlinear\n",
    "import nnts.torch.utils\n",
    "\n",
    "params = nnts.torch.models.tsmixer.get_mutlivariate_params()\n",
    "params.enc_in = 1\n",
    "params.individual = False\n",
    "\n",
    "dataset_options = {\n",
    "    \"context_length\": metadata.context_length,\n",
    "    \"prediction_length\": metadata.prediction_length,\n",
    "    \"conts\": [\n",
    "         \"HUFL\",\n",
    "         \"HULL\",\n",
    "         \"MUFL\",\n",
    "         \"MULL\",\n",
    "         \"LUFL\",\n",
    "         \"LULL\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "trn_dl, val_dl, test_dl = nnts.torch.utils.create_dataloaders_from_split_data(\n",
    "    split_data,\n",
    "    #Dataset=nnts.torch.datasets.TimeseriesDataset,\n",
    "    Dataset=nnts.torch.datasets.MultivariateTimeSeriesDataset,\n",
    "    dataset_options=dataset_options,\n",
    "    #Sampler=nnts.torch.datasets.TimeSeriesSampler,\n",
    "    batch_size=params.batch_size,\n",
    "    transforms = [nnts.torch.preprocessing.StandardScaler()]\n",
    ")\n",
    "\n",
    "net = nnts.torch.models.TSMixer(metadata.context_length, metadata.prediction_length,7, params)\n",
    "\n",
    "\n",
    "#net = nnts.torch.models.DLinear(\n",
    "#    metadata.prediction_length,\n",
    "#    metadata.context_length,\n",
    "#    7,\n",
    "#    params\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSMixer(\n",
      "  (mixer_layers): Sequential(\n",
      "    (0): MixerLayer(\n",
      "      (time_mixing): TimeMixing(\n",
      "        (norm): TimeBatchNorm2d(2352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.9, inplace=False)\n",
      "        (fc1): Linear(in_features=336, out_features=336, bias=True)\n",
      "      )\n",
      "      (feature_mixing): FeatureMixing(\n",
      "        (norm_before): TimeBatchNorm2d(2352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm_after): Identity()\n",
      "        (dropout): Dropout(p=0.9, inplace=False)\n",
      "        (fc1): Linear(in_features=7, out_features=256, bias=True)\n",
      "        (fc2): Linear(in_features=256, out_features=7, bias=True)\n",
      "        (projection): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): MixerLayer(\n",
      "      (time_mixing): TimeMixing(\n",
      "        (norm): TimeBatchNorm2d(2352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.9, inplace=False)\n",
      "        (fc1): Linear(in_features=336, out_features=336, bias=True)\n",
      "      )\n",
      "      (feature_mixing): FeatureMixing(\n",
      "        (norm_before): TimeBatchNorm2d(2352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm_after): Identity()\n",
      "        (dropout): Dropout(p=0.9, inplace=False)\n",
      "        (fc1): Linear(in_features=7, out_features=256, bias=True)\n",
      "        (fc2): Linear(in_features=256, out_features=7, bias=True)\n",
      "        (projection): Identity()\n",
      "      )\n",
      "    )\n",
      "    (2): MixerLayer(\n",
      "      (time_mixing): TimeMixing(\n",
      "        (norm): TimeBatchNorm2d(2352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout): Dropout(p=0.9, inplace=False)\n",
      "        (fc1): Linear(in_features=336, out_features=336, bias=True)\n",
      "      )\n",
      "      (feature_mixing): FeatureMixing(\n",
      "        (norm_before): TimeBatchNorm2d(2352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm_after): Identity()\n",
      "        (dropout): Dropout(p=0.9, inplace=False)\n",
      "        (fc1): Linear(in_features=7, out_features=256, bias=True)\n",
      "        (fc2): Linear(in_features=256, out_features=7, bias=True)\n",
      "        (projection): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (temporal_projection): Linear(in_features=336, out_features=336, bias=True)\n",
      ")\n",
      "Epoch 1 Train Loss: 0.7986729741096497  Valid Loss: 1.451457142829895\n",
      "saving model to logs/best_model.pt\n",
      "Epoch 2 Train Loss: 0.624279260635376  Valid Loss: 1.3456631898880005\n",
      "saving model to logs/best_model.pt\n",
      "reducing lr [0.0025]\n",
      "Epoch 3 Train Loss: 0.5474315285682678  Valid Loss: 1.3186557292938232\n",
      "saving model to logs/best_model.pt\n",
      "reducing lr [0.00125]\n",
      "Epoch 4 Train Loss: 0.5098639726638794  Valid Loss: 1.2648389339447021\n",
      "saving model to logs/best_model.pt\n",
      "reducing lr [0.000625]\n",
      "Epoch 5 Train Loss: 0.4935968816280365  Valid Loss: 1.2538083791732788\n",
      "saving model to logs/best_model.pt\n",
      "reducing lr [0.0003125]\n",
      "Epoch 6 Train Loss: 0.4850325286388397  Valid Loss: 1.2357884645462036\n",
      "saving model to logs/best_model.pt\n",
      "reducing lr [0.00015625]\n",
      "reducing lr [7.8125e-05]\n",
      "reducing lr [3.90625e-05]\n",
      "Epoch 9 Train Loss: 0.47636881470680237  Valid Loss: 1.2299103736877441\n",
      "saving model to logs/best_model.pt\n",
      "reducing lr [1.953125e-05]\n",
      "reducing lr [9.765625e-06]\n"
     ]
    }
   ],
   "source": [
    "trner = nnts.torch.trainers.ValidationTorchEpochTrainer(net, params, metadata)\n",
    "evaluator = trner.train(trn_dl, val_dl)\n",
    "y_hat, y = evaluator.evaluate(\n",
    "    test_dl, metadata.prediction_length, metadata.context_length\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2545, 336, 7]), torch.Size([2545, 336, 7]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnts.metrics.calculate_seasonal_error(trn_dl, metadata.seasonality).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(pred, true):\n",
    "    return torch.mean(torch.abs(pred - true))\n",
    "\n",
    "\n",
    "def MSE(pred, true):\n",
    "    return torch.mean((pred - true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.65725952), tensor(0.77339315))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(y_hat, y), MSE(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mse': 0.7733930349349976,\n",
       " 'abs_error': 3934250.25,\n",
       " 'abs_target_sum': 4788612.5,\n",
       " 'abs_target_mean': 0.7999901175498962,\n",
       " 'seasonal_error': 0.4050425887107849,\n",
       " 'mean_mase': 1.622692346572876,\n",
       " 'mean_mape': 5.055292129516602,\n",
       " 'mean_smape': 1.336384654045105,\n",
       " 'mean_msmape': 1.0481796264648438,\n",
       " 'mean_mae': 0.6572595238685608,\n",
       " 'mean_rmse': 0.8372031450271606,\n",
       " 'median_mase': 1.5489574670791626,\n",
       " 'median_smape': 1.4468520879745483,\n",
       " 'median_msmape': 1.0248525142669678,\n",
       " 'median_mae': 0.6273937225341797,\n",
       " 'median_rmse': 0.7912198901176453}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_metrics = nnts.metrics.calc_metrics(\n",
    "    y_hat, y, nnts.metrics.calculate_seasonal_error(trn_dl, metadata.seasonality)\n",
    ")\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09299259328599963"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.3049468696117401**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnts.metrics.calc_metrics(y_hat[:, :1, :], y[:, :1, :], nnts.metrics.calculate_seasonal_error(trn_dl, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(y_hat, y, name):\n",
    "    torch.save(y_hat, f\"{PATH}/{name}_y_hat.pt\")\n",
    "    torch.save(y, f\"{PATH}/{name}_y.pt\")\n",
    "save_results(y_hat, y, scenario.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_name = f\"cov-1-pearsn-0.68-pl-{str(scenario.prediction_length)}-seed-{scenario.seed}\"\n",
    "covariate_y_hat = torch.load(f\"{PATH}/{covariate_name}_y_hat.pt\")\n",
    "covariate_y = torch.load(f\"{PATH}/{covariate_name}_y.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_forecast_horizon_metrics(y_hat, y, metadata, metric=\"mae\"):\n",
    "    forecast_horizon_metrics = []\n",
    "    for i in range(1, metadata.prediction_length):\n",
    "        metrics = nnts.metrics.calc_metrics(y[:, :i, :], y_hat[:, :i, :], metadata.freq, metadata.seasonality)\n",
    "        forecast_horizon_metrics.append(metrics[metric])\n",
    "    return forecast_horizon_metrics\n",
    "\n",
    "forecast_horizon_metrics = calculate_forecast_horizon_metrics(y_hat, y, metadata, \"mae\")\n",
    "covariate_forecast_horizon_metrics = calculate_forecast_horizon_metrics(covariate_y_hat, covariate_y, metadata, \"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(forecast_horizon_metrics, label='univariate')\n",
    "plt.plot(covariate_forecast_horizon_metrics, label='covariate (0.68)')\n",
    "plt.xlabel(\"Forecast Horizon\")\n",
    "plt.ylabel(\"Error (MAE)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_aggregator = nnts.datasets.CSVFileAggregator(PATH, \"results\")\n",
    "results = csv_aggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(metadata.prediction_length*50)['y'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_last = y_hat[:, :1, ...]\n",
    "y_last = y[:, :1, ...]\n",
    "df_test = df.tail(y_hat_last.shape[0])\n",
    "df_test[\"y_check\"] = y_last.squeeze()\n",
    "df_test[\"y_hat\"] = y_hat_last.squeeze()\n",
    "df_test[[\"y\", \"y_check\", \"y_hat\"]]\n",
    "df_test.set_index(\"ds\")[[\"y_check\", \"y_hat\"]].iloc[4500:4500+336].plot(figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = nnts.metrics.calc_metrics(y_last, y_hat_last, metadata.freq, metadata.seasonality)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnts-uwGNJDPB-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
