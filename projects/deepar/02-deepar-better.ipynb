{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import gluonts\n",
    "import numpy as np\n",
    "import nnts\n",
    "import nnts.data\n",
    "import nnts.experiments\n",
    "import nnts.models\n",
    "import nnts.torch.data.preprocessing\n",
    "import nnts.torch.models\n",
    "import trainers\n",
    "import nnts.metrics\n",
    "import nnts.torch.data\n",
    "import nnts.torch.data.datasets\n",
    "import nnts.loggers\n",
    "import nnts.pandas\n",
    "import nnts\n",
    "import nnts.experiments.plotting\n",
    "import deepar\n",
    "from deepar import LagScenario\n",
    "\n",
    "torch.set_printoptions(precision=8, sci_mode=False)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "model_name = \"bettter-deepar\"\n",
    "base_model_name = \"base-lstm\"\n",
    "dataset_name = \"tourism\"\n",
    "results_path = \"ablation-results\"\n",
    "metadata_path = os.path.join(data_path, f\"{base_model_name}-monash.json\")\n",
    "metadata = nnts.data.metadata.load(dataset_name, path=metadata_path)\n",
    "datafile_path = os.path.join(data_path, metadata.filename)\n",
    "PATH = os.path.join(results_path, model_name, metadata.dataset)\n",
    "\n",
    "df_orig, *_ = nnts.pandas.read_tsf(datafile_path)\n",
    "params = nnts.models.Hyperparams()\n",
    "\n",
    "nnts.loggers.makedirs_if_not_exists(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gluonts defaults\n",
    "params.batch_size = 32\n",
    "params.batches_per_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(df_orig: pd.DataFrame):\n",
    "    df_orig[\"day_of_week\"] = df_orig[\"ds\"].dt.day_of_week\n",
    "    df_orig[\"hour\"] = df_orig[\"ds\"].dt.hour\n",
    "    df_orig[\"week\"] = df_orig[\"ds\"].dt.isocalendar().week\n",
    "    df_orig[\"week\"] = df_orig[\"week\"].astype(np.float32)\n",
    "\n",
    "    df_orig[\"month\"] = df_orig[\"ds\"].dt.month\n",
    "    df_orig[\"month\"] = np.sin(df_orig[\"month\"] * 2 * np.pi / 12)\n",
    "\n",
    "    df_orig[\"unix_timestamp\"] = (\n",
    "        df_orig[\"ds\"] - pd.Timestamp(\"1970-01-01\")\n",
    "    ) // pd.Timedelta(\"1s\")\n",
    "\n",
    "    return df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = create_time_features(df_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GlounTS uses the following code to generate the month covariate used in the tourism dataset\n",
    "# the month value is extracted from the date column and then scaled to a value between -0.5 and 0.5\n",
    "# here we do this is on the whole dataset in one go\n",
    "max_min_scaler = nnts.torch.data.preprocessing.MaxMinScaler()\n",
    "max_min_scaler.fit(df_orig, [\"unix_timestamp\"])\n",
    "df_orig = max_min_scaler.transform(df_orig, [\"unix_timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_orig[df_orig['unique_id'] == 'T1'].set_index('ds').tail(36)['month'].plot(figsize=(20, 5))\n",
    "fig = ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "lag_seq = gluonts.time_feature.lag.get_lags_for_frequency(metadata.freq)\n",
    "lag_seq = [lag - 1 for lag in lag_seq if lag > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaled_covariates = [\"month\", \"unix_timestamp\", nnts.torch.models.deepar.FEAT_SCALE]\n",
    "\n",
    "scaled_covariate_selection_matrix = [\n",
    "    [0,0,1],\n",
    "    [0,1,0],\n",
    "    [0,1,1],\n",
    "    [1,0,0],\n",
    "    [1,0,1],\n",
    "    [1,1,0],\n",
    "    [1,1,1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_list: List[nnts.experiments.Scenario] = []\n",
    "\n",
    "for seed in [42, 43, 44, 45, 46]:\n",
    "    for row in scaled_covariate_selection_matrix:\n",
    "        selected_combination = [\n",
    "            covariate\n",
    "            for covariate, select in zip(scaled_covariates, row)\n",
    "            if select == 1\n",
    "        ]\n",
    "        scenario_list.append(\n",
    "            LagScenario(\n",
    "                metadata.prediction_length,\n",
    "                conts=[cov for cov in selected_combination if cov != nnts.torch.models.deepar.FEAT_SCALE],\n",
    "                scaled_covariates=selected_combination,\n",
    "                lag_seq=lag_seq,\n",
    "                seed=seed,\n",
    "                dataset=metadata.dataset,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASELINE\n",
    "scenario_list = []\n",
    "for seed in [42, 43, 44, 45, 46]:\n",
    "    scenario = LagScenario(\n",
    "        metadata.prediction_length,\n",
    "        conts=[],\n",
    "        scaled_covariates=[],\n",
    "        lag_seq=lag_seq,\n",
    "        seed=seed,\n",
    "        dataset=metadata.dataset,\n",
    "    )\n",
    "    scenario_list.append(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR uses Teacher Forcing but we can use Free Running\n",
    "params.training_method = nnts.models.hyperparams.TrainingMethod.FREE_RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in scenario_list:\n",
    "    nnts.torch.data.datasets.seed_everything(scenario.seed)\n",
    "    df = df_orig.copy()\n",
    "    context_length = metadata.context_length + max(scenario.lag_seq)\n",
    "    split_data = nnts.pandas.split_test_train_last_horizon(\n",
    "        df, context_length, metadata.prediction_length\n",
    "    )\n",
    "    trn_dl, test_dl = nnts.data.create_trn_test_dataloaders(\n",
    "        split_data,\n",
    "        metadata,\n",
    "        scenario,\n",
    "        params,\n",
    "        nnts.torch.data.preprocessing.TorchTimeseriesLagsDataLoaderFactory(),\n",
    "    )\n",
    "    logger = nnts.loggers.LocalFileRun(\n",
    "        project=f\"{model_name}-{metadata.dataset}\",\n",
    "        name=scenario.name,\n",
    "        config={\n",
    "            **params.__dict__,\n",
    "            **metadata.__dict__,\n",
    "            **scenario.__dict__,\n",
    "        },\n",
    "        path=PATH,\n",
    "    )\n",
    "    net = nnts.torch.models.DeepAR(\n",
    "        nnts.torch.models.LinearModel,\n",
    "        params,\n",
    "        nnts.torch.data.preprocessing.masked_mean_abs_scaling,\n",
    "        1,\n",
    "        lag_seq=lag_seq,\n",
    "        scaled_features=scenario.scaled_covariates,\n",
    "    )\n",
    "    trner = trainers.TorchEpochTrainer(\n",
    "        nnts.models.TrainerState(), \n",
    "        net, \n",
    "        params, \n",
    "        metadata, \n",
    "        os.path.join(PATH, f\"{scenario.name}.pt\")\n",
    "    )\n",
    "    logger.configure(trner.events)\n",
    "\n",
    "    evaluator = trner.train(trn_dl)\n",
    "    y_hat, y = evaluator.evaluate(\n",
    "        test_dl, scenario.prediction_length, metadata.context_length\n",
    "    )\n",
    "    test_metrics = nnts.metrics.calc_metrics(\n",
    "        y, y_hat, trn_dl, metadata\n",
    "    )\n",
    "    logger.log(test_metrics)\n",
    "    print(test_metrics)\n",
    "    logger.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_aggregator = nnts.pandas.CSVFileAggregator(PATH, \"results\")\n",
    "results = csv_aggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(f\"{PATH}/results.csv\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = deepar.add_y_hat(df, y_hat, scenario.prediction_length)\n",
    "sample_preds = nnts.experiments.plotting.plot(df_list, scenario.prediction_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
