{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples, num_features):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_features = num_features\n",
    "        # Generate dummy data\n",
    "        self.data = torch.randn(num_samples, num_features)\n",
    "        self.labels = torch.randint(\n",
    "            0, 2, (num_samples,)\n",
    "        )  # Binary labels for classification\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_samples = 1000\n",
    "num_features = 10\n",
    "batch_size = 32\n",
    "lr = 0.1\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dummy_dataset = DummyDataset(num_samples, num_features)\n",
    "data_loader = torch.utils.data.DataLoader(dummy_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(DummyModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "model = DummyModel(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=lr*3, steps_per_epoch=len(data_loader), epochs=10\n",
    ")\n",
    "\n",
    "lr_list = []\n",
    "for epoch in range(10):\n",
    "    for batch in data_loader:\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        lr_list.append(scheduler.get_last_lr())\n",
    "plt.plot(lr_list)\n",
    "# Add labels and title\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.title('Learning Rate Schedule Over Time')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "step_lr_list = []\n",
    "for epoch in range(100):\n",
    "    scheduler.step()\n",
    "    step_lr_list.append(scheduler.get_last_lr())\n",
    "plt.plot(step_lr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Annealing with Warm Restarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "iters = len(data_loader)\n",
    "lr_list = []\n",
    "for epoch in range(100):\n",
    "    for i, sample in enumerate(data_loader):\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + i / iters)\n",
    "        lr_list.append(scheduler.get_last_lr())\n",
    "plt.plot(lr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CyclicLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.lr_scheduler.CyclicLR\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=lr, max_lr=lr*3, step_size_up=100)\n",
    "lr_list = []\n",
    "for epoch in range(25):\n",
    "    for batch in data_loader:\n",
    "        scheduler.step()\n",
    "        lr_list.append(scheduler.get_last_lr())\n",
    "plt.plot(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "step_lr_list = []\n",
    "for epoch in range(100):\n",
    "    scheduler.step()\n",
    "    step_lr_list.append(scheduler.get_last_lr())\n",
    "plt.plot(step_lr_list)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10, T_mult=1\n",
    ")\n",
    "iters = len(data_loader)\n",
    "lr_list = []\n",
    "for epoch in range(100):\n",
    "    for i, sample in enumerate(data_loader):\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + i / iters)\n",
    "        lr_list.append(scheduler.get_last_lr())\n",
    "plt.plot(lr_list)\n",
    "\n",
    "\n",
    "torch.optim.lr_scheduler.CyclicLR\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "    optimizer, base_lr=lr, max_lr=lr * 3, step_size_up=100\n",
    ")\n",
    "lr_list = []\n",
    "for epoch in range(25):\n",
    "    for batch in data_loader:\n",
    "        scheduler.step()\n",
    "        lr_list.append(scheduler.get_last_lr())\n",
    "plt.plot(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `model`, `lr`, and `data_loader` are already defined\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# StepLR\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "step_lr_list = []\n",
    "for epoch in range(25 * len(data_loader)):\n",
    "    scheduler.step()\n",
    "    step_lr_list.append(scheduler.get_last_lr())\n",
    "\n",
    "# CosineAnnealingWarmRestarts\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10, T_mult=1\n",
    ")\n",
    "cosine_lr_list = []\n",
    "iters = len(data_loader)\n",
    "for epoch in range(25):\n",
    "    for i, sample in enumerate(data_loader):\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + i / iters)\n",
    "        cosine_lr_list.append(scheduler.get_last_lr())\n",
    "\n",
    "# CyclicLR\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "    optimizer, base_lr=lr / 3, max_lr=lr, step_size_up=100\n",
    ")\n",
    "cyclic_lr_list = []\n",
    "for epoch in range(25):\n",
    "    for batch in data_loader:\n",
    "        scheduler.step()\n",
    "        cyclic_lr_list.append(scheduler.get_last_lr())\n",
    "\n",
    "# Flatten the lists of learning rates\n",
    "step_lr_list_flat = [item for sublist in step_lr_list for item in sublist]\n",
    "cosine_lr_list_flat = [item for sublist in cosine_lr_list for item in sublist]\n",
    "cyclic_lr_list_flat = [item for sublist in cyclic_lr_list for item in sublist]\n",
    "\n",
    "# Plot all learning rates on the same plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(step_lr_list_flat, label=\"StepLR\")\n",
    "plt.plot(cosine_lr_list_flat, label=\"CosineAnnealingWarmRestarts\")\n",
    "plt.plot(cyclic_lr_list_flat, label=\"CyclicLR\")\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.title(\"Learning Rate Schedules\")\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `model`, `lr`, and `data_loader` are already defined\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Create a figure with 3 subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# StepLR\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.1)\n",
    "step_lr_list = []\n",
    "for epoch in range(25 * len(data_loader)):\n",
    "    scheduler.step()\n",
    "    step_lr_list.append(scheduler.get_last_lr())\n",
    "\n",
    "# Flatten the list of learning rates\n",
    "step_lr_list_flat = [item for sublist in step_lr_list for item in sublist]\n",
    "\n",
    "# Plot StepLR\n",
    "axs[0].plot(step_lr_list_flat)\n",
    "axs[0].set_title(\"StepLR\")\n",
    "axs[0].set_xlabel(\"Iteration\")\n",
    "axs[0].set_ylabel(\"Learning rate\")\n",
    "\n",
    "# CosineAnnealingWarmRestarts\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10, T_mult=1\n",
    ")\n",
    "cosine_lr_list = []\n",
    "iters = len(data_loader)\n",
    "for epoch in range(25):\n",
    "    for i, sample in enumerate(data_loader):\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + i / iters)\n",
    "        cosine_lr_list.append(scheduler.get_last_lr())\n",
    "\n",
    "# Flatten the list of learning rates\n",
    "cosine_lr_list_flat = [item for sublist in cosine_lr_list for item in sublist]\n",
    "\n",
    "# Plot CosineAnnealingWarmRestarts\n",
    "axs[1].plot(cosine_lr_list_flat)\n",
    "axs[1].set_title(\"CosineAnnealingWarmRestarts\")\n",
    "axs[1].set_xlabel(\"Iteration\")\n",
    "axs[1].set_ylabel(\"Learning rate\")\n",
    "\n",
    "# CyclicLR\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "    optimizer, base_lr=lr /3, max_lr=lr, step_size_up=100\n",
    ")\n",
    "cyclic_lr_list = []\n",
    "for epoch in range(25):\n",
    "    for batch in data_loader:\n",
    "        scheduler.step()\n",
    "        cyclic_lr_list.append(scheduler.get_last_lr())\n",
    "\n",
    "# Flatten the list of learning rates\n",
    "cyclic_lr_list_flat = [item for sublist in cyclic_lr_list for item in sublist]\n",
    "\n",
    "# Plot CyclicLR\n",
    "axs[2].plot(cyclic_lr_list_flat)\n",
    "axs[2].set_title(\"CyclicLR\")\n",
    "axs[2].set_xlabel(\"Iteration\")\n",
    "axs[2].set_ylabel(\"Learning rate\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
