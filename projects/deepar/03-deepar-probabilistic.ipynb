{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import gluonts\n",
    "import numpy as np\n",
    "import nnts\n",
    "import nnts.data\n",
    "import nnts.experiments\n",
    "import nnts.models\n",
    "import nnts.torch.data.preprocessing\n",
    "import nnts.torch.models\n",
    "import trainers\n",
    "import nnts.metrics\n",
    "import nnts.torch.data\n",
    "import nnts.torch.data.datasets\n",
    "import nnts.loggers\n",
    "import nnts.pandas\n",
    "import nnts\n",
    "import nnts.experiments.plotting\n",
    "import projects.deepar.runner as runner\n",
    "from projects.deepar.runner import LagScenario\n",
    "\n",
    "torch.set_printoptions(precision=8, sci_mode=False)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "model_name = \"distr-deepar\"\n",
    "base_model_name = \"base-lstm\"\n",
    "dataset_name = \"tourism\"\n",
    "results_path = \"ablation-results\"\n",
    "metadata_path = os.path.join(data_path, f\"{base_model_name}-monash.json\")\n",
    "metadata = nnts.data.metadata.load(dataset_name, path=metadata_path)\n",
    "datafile_path = os.path.join(data_path, metadata.filename)\n",
    "PATH = os.path.join(results_path, model_name, metadata.dataset)\n",
    "\n",
    "df_orig, *_ = nnts.pandas.read_tsf(datafile_path)\n",
    "params = nnts.models.Hyperparams()\n",
    "params.optimizer = torch.optim.Adam\n",
    "\n",
    "nnts.loggers.makedirs_if_not_exists(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gluonts defaults\n",
    "params.batch_size = 32\n",
    "params.batches_per_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(df_orig: pd.DataFrame):\n",
    "    df_orig[\"day_of_week\"] = df_orig[\"ds\"].dt.day_of_week\n",
    "    df_orig[\"hour\"] = df_orig[\"ds\"].dt.hour\n",
    "    df_orig[\"week\"] = df_orig[\"ds\"].dt.isocalendar().week\n",
    "    df_orig[\"week\"] = df_orig[\"week\"].astype(np.float32)\n",
    "\n",
    "    df_orig[\"month\"] = df_orig[\"ds\"].dt.month\n",
    "\n",
    "    df_orig[\"unix_timestamp\"] = np.log10(2.0 + df_orig.groupby(\"unique_id\").cumcount())\n",
    "\n",
    "    return df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = create_time_features(df_orig)\n",
    "\n",
    "\n",
    "def create_dummy_unique_ids(df_orig: pd.DataFrame):\n",
    "    df_orig[\"unique_id_0\"] = 0.0\n",
    "    df_orig[\"unique_id_1\"] = 0.0597\n",
    "    return df_orig\n",
    "\n",
    "\n",
    "df_orig = create_dummy_unique_ids(df_orig)\n",
    "\n",
    "# Normalize data\n",
    "max_min_scaler = nnts.torch.data.preprocessing.MaxMinScaler()\n",
    "max_min_scaler.fit(df_orig, [\"month\"])\n",
    "df_orig = max_min_scaler.transform(df_orig, [\"month\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GlounTS uses the following code to generate the month covariate used in the tourism dataset\n",
    "# the month value is extracted from the date column and then scaled to a value between -0.5 and 0.5\n",
    "# here we do this is on the whole dataset in one go\n",
    "max_min_scaler = nnts.torch.data.preprocessing.MaxMinScaler()\n",
    "max_min_scaler.fit(df_orig, [\"unix_timestamp\"])\n",
    "df_orig = max_min_scaler.transform(df_orig, [\"unix_timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_orig[df_orig['unique_id'] == 'T1'].set_index('ds').tail(36)['month'].plot(figsize=(20, 5))\n",
    "fig = ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "lag_seq = gluonts.time_feature.lag.get_lags_for_frequency(metadata.freq)\n",
    "lag_seq = [lag - 1 for lag in lag_seq if lag > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaled_covariates = [\"month\", \"unix_timestamp\", nnts.torch.models.deepar.FEAT_SCALE]\n",
    "\n",
    "scaled_covariate_selection_matrix = [\n",
    "    [0,0,1],\n",
    "    [0,1,0],\n",
    "    [0,1,1],\n",
    "    [1,0,0],\n",
    "    [1,0,1],\n",
    "    [1,1,0],\n",
    "    [1,1,1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_list: List[nnts.experiments.Scenario] = []\n",
    "\n",
    "for seed in [42, 43, 44, 45, 46]:\n",
    "    for row in scaled_covariate_selection_matrix:\n",
    "        selected_combination = [\n",
    "            covariate\n",
    "            for covariate, select in zip(scaled_covariates, row)\n",
    "            if select == 1\n",
    "        ]\n",
    "        scenario_list.append(\n",
    "            LagScenario(\n",
    "                metadata.prediction_length,\n",
    "                conts=[cov for cov in selected_combination if cov != nnts.torch.models.deepar.FEAT_SCALE],\n",
    "                scaled_covariates=selected_combination,\n",
    "                lag_seq=lag_seq,\n",
    "                seed=seed,\n",
    "                dataset=metadata.dataset,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE\n",
    "conts = [\n",
    "    \"month\",\n",
    "    \"unix_timestamp\",\n",
    "    \"unique_id_0\",\n",
    "]\n",
    "scenario_list = []\n",
    "for seed in [42, 43, 44, 45, 46]:\n",
    "    scenario = LagScenario(\n",
    "        metadata.prediction_length,\n",
    "        conts=conts,\n",
    "        scaled_covariates=conts+ [\n",
    "                nnts.torch.models.deepar.FEAT_SCALE,\n",
    "            ],\n",
    "        lag_seq=lag_seq,\n",
    "        seed=seed,\n",
    "        dataset=metadata.dataset,\n",
    "    )\n",
    "    scenario_list.append(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR uses Teacher Forcing but we can use Free Running\n",
    "params.training_method = nnts.models.hyperparams.TrainingMethod.TEACHER_FORCING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.distributions as td\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StudentTHead(nn.Module):\n",
    "    \"\"\"\n",
    "    This model outputs a studentT distribution.\n",
    "    \"\"\"\n",
    "    PARAMS = 2\n",
    "    def __init__(self, hidden_size: int, output_size: int):\n",
    "        super(StudentTHead, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size*StudentTHead.PARAMS),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor, target_scale: torch.tensor):\n",
    "        y_hat = self.main(x)\n",
    "        df = F.softplus(y_hat[..., 0:1])\n",
    "        loc = y_hat[..., 1:2]\n",
    "        scale = target_scale if target_scale is not None else 1\n",
    "        return td.StudentT(df, loc, scale)\n",
    "    \n",
    "\n",
    "def distr_nll(distr: td.Distribution, target: torch.Tensor) -> torch.Tensor:\n",
    "    return -distr.log_prob(target).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in scenario_list:\n",
    "    nnts.torch.data.datasets.seed_everything(scenario.seed)\n",
    "    df = df_orig.copy()\n",
    "    context_length = metadata.context_length + max(scenario.lag_seq)\n",
    "    split_data = nnts.pandas.split_test_train_last_horizon(\n",
    "        df, context_length, metadata.prediction_length\n",
    "    )\n",
    "    trn_dl, test_dl = nnts.data.create_trn_test_dataloaders(\n",
    "        split_data,\n",
    "        metadata,\n",
    "        scenario,\n",
    "        params,\n",
    "        nnts.torch.data.datasets.TorchTimeseriesLagsDataLoaderFactory(),\n",
    "    )\n",
    "    logger = nnts.loggers.LocalFileRun(\n",
    "        project=f\"{model_name}-{metadata.dataset}\",\n",
    "        name=scenario.name,\n",
    "        config={\n",
    "            **params.__dict__,\n",
    "            **metadata.__dict__,\n",
    "            **scenario.__dict__,\n",
    "        },\n",
    "        path=PATH,\n",
    "    )\n",
    "    net = nnts.torch.models.DistrDeepAR(\n",
    "        StudentTHead,\n",
    "        params,\n",
    "        nnts.torch.data.preprocessing.masked_mean_abs_scaling,\n",
    "        1,\n",
    "        lag_seq=lag_seq,\n",
    "        scaled_features=scenario.scaled_covariates,\n",
    "    )\n",
    "    trner = trainers.TorchEpochTrainer(\n",
    "        nnts.models.TrainerState(), \n",
    "        net, \n",
    "        params, \n",
    "        metadata, \n",
    "        os.path.join(PATH, f\"{scenario.name}.pt\"),\n",
    "        loss_fn=distr_nll\n",
    "    )\n",
    "    logger.configure(trner.events)\n",
    "\n",
    "    evaluator = trner.train(trn_dl)\n",
    "    y_hat, y = evaluator.evaluate(\n",
    "        test_dl, scenario.prediction_length, metadata.context_length\n",
    "    )\n",
    "    test_metrics = nnts.metrics.calc_metrics(\n",
    "        y, y_hat, nnts.metrics.calculate_seasonal_error(trn_dl, metadata)\n",
    "    )\n",
    "    logger.log(test_metrics)\n",
    "    print(test_metrics)\n",
    "    logger.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_aggregator = nnts.pandas.CSVFileAggregator(PATH, \"results\")\n",
    "results = csv_aggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(f\"{PATH}/results.csv\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['smape', 'mase']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = runner.add_y_hat(df, y_hat, scenario.prediction_length)\n",
    "sample_preds = nnts.experiments.plotting.plot(df_list, scenario.prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.repository import dataset_names, get_dataset\n",
    "\n",
    "dataset = get_dataset(\"tourism_monthly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonts.evaluation.metrics\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_list = []\n",
    "for batch in dataset.train:\n",
    "    past_data = batch[\"target\"]\n",
    "    se = gluonts.evaluation.metrics.calculate_seasonal_error(past_data, \"1M\", 12)\n",
    "    se_list.append(se)\n",
    "torch.tensor(se_list).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gluonadapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.load(\n",
    "    \"/Users/garethdavies/Development/workspaces/gluonts/examples/input.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = gluonadapt.get_test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in scenario_list[:1]:\n",
    "    nnts.torch.data.datasets.seed_everything(scenario.seed)\n",
    "    df = df_orig.copy()\n",
    "    context_length = metadata.context_length + max(scenario.lag_seq)\n",
    "    split_data = nnts.pandas.split_test_train_last_horizon(\n",
    "        df, context_length, metadata.prediction_length\n",
    "    )\n",
    "    trn_dl, test_dl = nnts.data.create_trn_test_dataloaders(\n",
    "        split_data,\n",
    "        metadata,\n",
    "        scenario,\n",
    "        params,\n",
    "        nnts.torch.data.datasets.TorchTimeseriesLagsDataLoaderFactory(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(batch[\"X\"][:, :51, 2], dl[0][\"X\"][:, :51, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(batch['X'][10, :51, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dl[0]['X'][10, :51, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl[0]['X'][0, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl[0]['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\n",
    "    \"/Users/garethdavies/Development/workspaces/nnts/projects/deepar/gluonts.pt\"\n",
    ")\n",
    "rnn = {k: v for k, v in state_dict.items() if k.startswith(\"rnn\")}\n",
    "net.decoder.load_state_dict(rnn)\n",
    "net.embbeder.load_state_dict({\"weight\": state_dict[\"embedder._embedders.0.weight\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in state_dict.items() if not k.startswith(\"rnn\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.float_format = \"{:.3f}\".format\n",
    "df = pd.read_csv(\"../better_deepar/ablation-results/better-deepar-studentt/traffic_hourly/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse                  0.001\n",
       "abs_error         2474.341\n",
       "abs_target_sum    6186.671\n",
       "abs_target_mean      0.043\n",
       "mase                 1.087\n",
       "mape                   inf\n",
       "smape                0.399\n",
       "nd                     inf\n",
       "mae                  0.017\n",
       "rmse                 0.028\n",
       "seasonal_error       0.015\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\n",
    "    \"mse\",\n",
    "    \"abs_error\",\n",
    "    \"abs_target_sum\",\n",
    "    \"abs_target_mean\",\n",
    "    \"mase\",\n",
    "    \"mape\",\n",
    "    \"smape\",\n",
    "    \"nd\",\n",
    "    \"mae\",\n",
    "    \"rmse\",\n",
    "    \"seasonal_error\",\n",
    "]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x1468f81d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "sample_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.randn(10, 5), torch.randint(0, 2, (10,))\n",
    ")\n",
    "sample_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
